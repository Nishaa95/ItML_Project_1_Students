{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1 - NLP and Text Classification\n",
    "\n",
    "For this project you will need to classify some angry comments into their respective category of angry. The process that you'll need to follow is (roughly):\n",
    "<ol>\n",
    "<li> Use NLP techniques to process the training data. \n",
    "<li> Train model(s) to predict which class(es) each comment is in.\n",
    "    <ul>\n",
    "    <li> A comment can belong to any number of classes, including none. \n",
    "    </ul>\n",
    "<li> Generate predictions for each of the comments in the test data. \n",
    "<li> Write your test data predicitions to a CSV file, which will be scored. \n",
    "</ol>\n",
    "\n",
    "You can use any models and NLP libraries you'd like. Think aobut the problem, look back to see if there's anything that might help, give it a try, and see if that helps. We've regularly said we have a \"toolkit\" of things that we can use, we generally don't know which ones we'll need, but here you have a pretty simple goal - if it makes it more accurate, it helps. There's not one specific solution here, there are lots of things that you could do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "Use the training data to train your prediction model(s). Each of the classification output columns (toxic to the end) is a human label for the comment_text, assessing if it falls into that category of \"rude\". A comment may fall into any number of categories, or none at all. Membership in one output category is <b>independent</b> of membership in any of the other classes (think about this when you plan on how to make these predictions - it may also make it easier to split work amongst a team...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv.zip\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"non_toxic\"] = train_df.iloc[:,2:8].apply(lambda x: 1 if (sum(x)==0) else 0, axis=1)\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total={}\n",
    "for col in train_df.iloc[:,2:].columns:\n",
    "    total[col]=train_df[col].value_counts()[1]\n",
    "    print(f\"{col}: {total[col]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['classes']=train_df.iloc[:,2:8].sum(axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_totals={}\n",
    "for label in train_df['classes'].unique():\n",
    "    label_totals[label]=train_df['classes'].value_counts()[label]\n",
    "\n",
    "label_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=[key for key in label_totals.keys()]\n",
    "values=[key for key in label_totals.values()]\n",
    "keys1=keys.copy()\n",
    "values1=values.copy()\n",
    "keys1.pop(0)\n",
    "values1.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "fr = plt.subplot(2,1,1)\n",
    "plt.bar(keys,values)\n",
    "\n",
    "fr.set_xlabel(\"Total Number of Comments\", fontsize=15)\n",
    "fr.set_ylabel(\"Number of Classes comments belong to\", fontsize=15)\n",
    "fr.set_title(\"Plot including unlabeled(0) comments \", fontsize=17)\n",
    "\n",
    "\n",
    "se = plt.subplot(2,1,2)\n",
    "plt.bar(keys1, values1)\n",
    "\n",
    "se.set_xlabel(\"Total Number of Comments\", fontsize=15)\n",
    "se.set_ylabel(\"Number of Classes comments belong to\", fontsize=15)\n",
    "se.set_title(\"Plot excluding unlabeled(0) comments\", fontsize=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "rude=list(total.values())\n",
    "\n",
    "fr = plt.subplot(2,1,1)\n",
    "plt.bar(x = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate','non_toxic'],height=total.values())\n",
    "\n",
    "fr.set_xlabel(\"Total Number of Comments\", fontsize=15)\n",
    "fr.set_ylabel(\"Number of Classes comments belong to\", fontsize=15)\n",
    "fr.set_title(\"distribution of classes with non toxic values\", fontsize=17)\n",
    "\n",
    "\n",
    "se = plt.subplot(2,1,2)\n",
    "plt.bar(x = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate'], height=rude[:-1])\n",
    "\n",
    "se.set_xlabel(\"Total Number of Comments\", fontsize=15)\n",
    "se.set_ylabel(\"Number of Classes comments belong to\", fontsize=15)\n",
    "se.set_title(\"distribution of classes without non toxic values\", fontsize=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data=train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['comment_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess(text):\n",
    "\n",
    "    #Removing IP address\n",
    "    \n",
    "    text=re.sub(r\"([0-9]{1,3})\\.([0-9]{1,3})\\.([0-9]{1,3})\\.([0-9]{1,3})\",\"\",text)\n",
    "\n",
    "    #Getting ride of the URLs with space even if they are in paranthesis \n",
    "\n",
    "    text=re.sub(r\"\\S*https?:\\S*|\\s*www\\.\\s*\", \"\", text)\n",
    "\n",
    "    text=re.sub(r\"\\\"\\\"\", \"\\\"\",text)  # replacing \"\" with \"\n",
    "\n",
    "    text=re.sub(r\"^\\\"\", \"\",text)      # removing quotation from start and the end of the string\n",
    "\n",
    "    #Remove special characters and numbers \n",
    "    text=re.sub(r'\\s*[^a-zA-Z]\\s*',' ',text)\n",
    "\n",
    "    #Removing extra spaces in the text \n",
    "    text=re.sub(r\"\\s\\s+\", \" \",text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['comment_text']=processed_data['comment_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working_data=processed_data.copy()\n",
    "#C_indexs = processed_data[processed_data[\"classes\"]==0].index\n",
    "#C_indexs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep 5000 of non_toxic comments and drop the rest\n",
    "#drop_ind=np.random.choice(C_indexs, size=138346, replace=False)\n",
    "#working_data.drop(drop_ind, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph(A,B):\n",
    "    classes_A={}\n",
    "    for col in A.iloc[:,1:8].columns:\n",
    "        classes_A[col]=A[col].value_counts()[1]\n",
    "    classes_B={}\n",
    "    for col in B.iloc[:,1:8].columns:\n",
    "        classes_B[col]=B[col].value_counts()[1]\n",
    "    multi_class_A={}\n",
    "    for value in A['classes'].unique():\n",
    "        multi_class_A[value]=A['classes'].value_counts()[value]\n",
    "    multi_class_B={}\n",
    "    for value in B['classes'].unique():\n",
    "        multi_class_B[value]=B['classes'].value_counts()[value]\n",
    "\n",
    "    key_A=[key for key in multi_class_A.keys()]\n",
    "    values_A=[key for key in multi_class_A.values()]\n",
    "\n",
    "    key_B=[key for key in multi_class_B.keys()]\n",
    "    values_B=[key for key in multi_class_B.values()]\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "    t_left=plt.subplot(2,2,1)\n",
    "    plt.bar(x=['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate','non_toxic'], height=classes_A.values())\n",
    "    t_left.set_title(\"Distribution of classes before \\ndeleting non toxic samples\", fontsize=22)\n",
    "    t_left.set_xlabel(\"Classes\", fontsize=15)\n",
    "    t_left.set_ylabel(\"Frequency\", fontsize=15)\n",
    "    t_right=plt.subplot(2,2,2)\n",
    "    plt.bar(x=['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate','non_toxic'], height=classes_B.values())\n",
    "    t_right.set_title(\"Distribution of classes After \\ndeleting non toxic samples\", fontsize=22)\n",
    "    t_right.set_xlabel(\"Classes\", fontsize=15)\n",
    "    t_right.set_ylabel(\"Frequency\", fontsize=15)\n",
    "    d_left=plt.subplot(2,2,3)\n",
    "    plt.bar(x=key_A, height=values_A)\n",
    "    d_left.set_title(\"Distribution of total labeled comments before \\ndeleting non toxic samples\", fontsize=22)\n",
    "    d_left.set_xlabel(\"Classes\", fontsize=15)\n",
    "    d_left.set_ylabel(\"Frequency\", fontsize=15)\n",
    "    d_right=plt.subplot(2,2,4)\n",
    "    plt.bar(x=key_B, height=values_B)\n",
    "    d_right.set_title(\"Distribution of total labeled comments after \\ndeleting non toxic samples\", fontsize=22)\n",
    "    d_right.set_xlabel(\"Classes\", fontsize=15)\n",
    "    d_right.set_ylabel(\"Frequency\", fontsize=15)\n",
    "    \n",
    "\n",
    "Graph(processed_data,working_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data=working_data.sample(5000)\n",
    "working_data=working_data.reset_index()\n",
    "working_data.drop(columns=['index','non_toxic','classes'], inplace=True)\n",
    "working_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the data into test and train sets\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X=working_data.iloc[:,1]\n",
    "#y=working_data.iloc[:,2:8]\n",
    "\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                tok = re.sub('\\W+','', tok) #Punctuation strip\n",
    "                tmp = self.lemmatizer.lemmatize(tok)\n",
    "                if len(tmp) >= 2:\n",
    "                    filtered_tok.append(tmp)\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stemTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import SnowballStemmer\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(self.stemmer.stem(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(tok)\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#TF-IDF\n",
    "\n",
    "y = working_data.iloc[:,2:]\n",
    "X = working_data[\"comment_text\"]\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\",max_features=5000)\n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "# Train a logistic regression classifier for each toxicity label\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for label in labels:\n",
    "    y_train_label = y_train[label]\n",
    "    y_test_label = y_test[label]\n",
    "    pipe_steps1 = [(\"model\", SVC())]\n",
    "    pipe_test1 = Pipeline(steps=pipe_steps1)\n",
    "    pipe_test1.fit(X_train, y_train_label)\n",
    "    pipe_test1.score(X_test, y_test_label)\n",
    "    y_pred1 = pipe_test1.predict(X_test)\n",
    "    accuracy1 = accuracy_score(y_test_label, y_pred1)\n",
    "    print(f\"Accuracy score of {label}: {accuracy1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for label in labels:\n",
    "    y_train_label = y_train[label]\n",
    "    y_test_label = y_test[label]\n",
    "    svd_tmp2 = TruncatedSVD(n_components=20)\n",
    "    pipe_steps2 = [(\"svd\", svd_tmp2), (\"model\", SVC())]\n",
    "    pipe_test2 = Pipeline(steps=pipe_steps2)\n",
    "    pipe_test2.fit(X_train, y_train_label)\n",
    "    pipe_test2.score(X_test, y_test_label)\n",
    "    y_pred2 = pipe_test2.predict(X_test)\n",
    "    accuracy2 = accuracy_score(y_test_label, y_pred2)\n",
    "    print(f\"Accuracy score of {label}: {accuracy2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for label in labels:\n",
    "    y_train_label = y_train[label]\n",
    "    y_test_label = y_test[label]\n",
    "    svd_tmp3 = TruncatedSVD(n_components=20)\n",
    "    pipe_steps3 =[ ('svd', svd_tmp3), ('m', RandomForestClassifier())]\n",
    "    pipe_test3 = Pipeline(steps=pipe_steps3)\n",
    "    pipe_test3.fit(X_train, y_train_label)\n",
    "    pipe_test3.score(X_test, y_test_label)\n",
    "    y_pred3 = pipe_test3.predict(X_test)\n",
    "    accuracy3 = accuracy_score(y_test_label, y_pred3)\n",
    "    print(f\"Accuracy score of {label}: {accuracy3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for label in labels:\n",
    "    y_train_label = y_train[label]\n",
    "    y_test_label = y_test[label]\n",
    "    clf4 = LogisticRegression()\n",
    "    clf4.fit(X_train, y_train_label)\n",
    "    y_pred4 = clf4.predict(X_test)\n",
    "    accuracy4 = accuracy_score(y_test_label, y_pred4)\n",
    "    print(f\"Accuracy score of {label}: {accuracy4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for label in labels:\n",
    "    y_train_label = y_train[label]\n",
    "    y_test_label = y_test[label]\n",
    "    model6 = LogisticRegression(n_jobs=-1, max_iter=10000)\n",
    "    params6 = {}\n",
    "    \n",
    "    clf6 = GridSearchCV(model6, param_grid=params6, cv=3, n_jobs=-1)\n",
    "    clf6.fit(X_train, y_train_label)\n",
    "    y_pred6 = clf6.predict(X_test)\n",
    "    accuracy6 = accuracy_score(y_test_label, y_pred6)\n",
    "    print(f\"Accuracy score of {label}: {accuracy6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for label in labels:\n",
    "    y_train_label = y_train[label]\n",
    "    y_test_label = y_test[label]\n",
    "    model7=SVC()\n",
    "    params7 = {\"vect__max_features\":[100,500,1000,1500,2000,2500],\n",
    "            \"vect__tokenizer\":(swTokenizer(stop_words), stemTokenizer(stop_words), lemmaTokenizer(stop_words) ),\n",
    "            \"vect__norm\":[\"l1\",\"l2\"}\n",
    "\n",
    "    clf = GridSearchCV(estimator  = model7, param_grid = params7, scoring= \"balanced_accuracy\",\n",
    "                               cv= 5,n_jobs=-1)\n",
    "    clf.fit(X_train, y_train_label)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test_label, y_pred)\n",
    "    print(f\"Accuracy score of {label}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Details, Submission Info, and Example Submission\n",
    "\n",
    "For this project, please output your predictions in a CSV file. The structure of the CSV file should match the structure of the example below. \n",
    "\n",
    "The output should contain one row for each row of test data, complete with the columns for ID and each classification.\n",
    "\n",
    "Into Moodle please submit:\n",
    "<ul>\n",
    "<li> Your notebook file(s). I'm not going to run them, just look. \n",
    "<li> Your sample submission CSV. This will be evaluated for accuracy against the real labels; only a subset of the predictions will be scored. \n",
    "</ul>\n",
    "\n",
    "It is REALLY, REALLY, REALLY important the the structure of your output matches the specifications. The accuracies will be calculated by a script, and it is expecting a specific format. \n",
    "\n",
    "### Sample Evaluator\n",
    "\n",
    "The file prediction_evaluator.ipynb contains an example scoring function, scoreChecker. This function takes a sumbission and an answer key, loops through, and evaluates the accuracy. You can use this to verify the format of your submission. I'm going to use the same function to evaluate the accuracy of your submission, against the answer key (unless I made some mistake in this counting function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained classifiers\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "classifiers = {}\n",
    "for label in labels:\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train[label])\n",
    "    classifiers[label] = clf\n",
    "    classifiers[label] = (clf, X_train['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['comment_text'] = test_df['comment_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_test = vectorizer.fit_transform(test_df['comment_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the toxicity labels for the test data using the trained classifiers\n",
    "pred_labels = {}\n",
    "for label in labels:\n",
    "    clf = classifiers[label]\n",
    "    pred_labels[label] = clf.predict(X_test)\n",
    "    classifiers[label] = (clf, X_train['id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predicted labels in a file named out.csv\n",
    "out_df = pd.DataFrame(pred_labels, columns=labels)\n",
    "out_df.to_csv('Pre2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "The grading for this is split between accuracy and well written code:\n",
    "<ul>\n",
    "<li> 75% - Accuracy. The most accurate will get 100% on this, the others will be scaled down from there. \n",
    "<li> 25% - Code quality. Can the code be followed and made sense of - i.e. comments, sections, titles. \n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
